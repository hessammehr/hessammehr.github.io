<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title></title>
    <link
      rel="alternate"
      type="application/rss+xml"
      title="Hessam's blog RSS Feed"
      href="/feed.xml"
    />
    <link rel="stylesheet" href="/style.css" />
    <link rel="stylesheet" href="/primer.css" />
    <link
      rel="stylesheet"
      href="/light.css"
      media="(prefers-color-scheme: light)"
    />
    <link
      rel="stylesheet"
      href="/dark.css"
      media="(prefers-color-scheme: dark)"
    />
  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body"><h1
id="reparameterizing-distributions-in-numpyro">Reparameterizing
distributions in numpyro</h1>
<p>Every single time I’ve tried to “chance” it with probabilistic
methods I am once again reminded that you can’t treat them like a black
box. The following is a self-contained example of a pathological case so
simple it almost feels like it should just work, and yet it fails
miserably without intervention (in this case by reparameterizing the
distributions). Part of me wonders if this all just means we need better
tools or whether we just have to accept the fundamental complexity and
model defensively.</p>
<pre class="python"><code>import seaborn as sns
from jax import lax
from jax import numpy as jnp
from jax.random import PRNGKey
from matplotlib import pyplot as plt
from numpyro import deterministic, handlers, sample
from numpyro import distributions as dist
from numpyro.infer import MCMC, NUTS, SVI, Trace_ELBO, autoguide, reparam
from numpyro.optim import Adam

sns.set_theme(&#39;talk&#39;, &#39;ticks&#39;, font=&#39;Arial&#39;, font_scale=1.0, rc={&#39;svg.fonttype&#39;: &#39;none&#39;})</code></pre>
<h2 id="problem-setting">Problem setting</h2>
<p>Consider a catalyzed reaction <span class="math inline">$A
\xrightarrow{\mathrm{cat}} \cdots$</span> where the catalyst itself is
slowly undergoing decomposition <span
class="math inline">cat → ⋯</span>. Initial concentrations <span
class="math inline">[<em>A</em>]<sub>0</sub></span> and</p>
<p><span class="math inline">$\frac{d [A]}{d t} =
-k[A][\mathrm{cat}]$</span><br />
<span class="math inline">$\frac{d [\mathrm{cat}]}{d t} =
-k_d[\mathrm{cat}]$</span></p>
<p>A couple of simple helper functions to integrate these differential
equations.</p>
<pre class="python"><code>def solve(a_0, k, cats, dt):
    return lax.scan(lambda a_n, cat: (a_n - dt * k * cat * a_n, a_n - dt * k * cat * a_n), a_0, cats)[1]

def solve_cat(cat_0, k_d, dt, n):
    return lax.scan(lambda cat_n, _: (cat_n - dt * k_d * cat_n, cat_n - dt * k_d * cat_n), cat_0, length=n)[1]
    

plt.plot(solve(1.0, 1.0, jnp.ones((1000,)), 0.01))
plt.plot(solve_cat(1.0, 1.0, 0.02, 1000))</code></pre>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x11cd56540&gt;]</code></pre>
<figure>
<img src="2025-11-26-reparam_files/2025-11-26-reparam_3_1.png"
alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<p>Now a simple model, first without worrying about measurement error.
We’ll use both MCMC and SVI.</p>
<pre class="python"><code>def model(a_0, cat_0, dt, n):
    k = sample(&#39;k&#39;, dist.LogNormal(0.0, 0.5))
    k_d = sample(&#39;k_d&#39;, dist.LogNormal(-3.0, 0.5))
    cats = deterministic(&#39;cat&#39;, solve_cat(cat_0, k_d, dt, n))
    a_n = deterministic(&#39;a_n&#39;, solve(a_0, k, cats, dt))

args = (1.0, 1.0, 0.01, 500)
mcmc = MCMC(NUTS(model), num_warmup=1000, num_samples=100)
mcmc.run(PRNGKey(0), *args)
mcmc_samples = mcmc.get_samples()

guide = autoguide.AutoNormal(model)
svi = SVI(model, guide, Adam(0.01), Trace_ELBO())
svi_result = svi.run(PRNGKey(1), 2000, *args)
svi_samples = guide.sample_posterior(PRNGKey(0), svi_result.params, sample_shape=(100,))</code></pre>
<pre><code>sample: 100%|██████████| 1100/1100 [00:00&lt;00:00, 2349.69it/s, 1 steps of size 8.00e-01. acc. prob=0.93]
100%|██████████| 2000/2000 [00:00&lt;00:00, 7287.78it/s, init loss: 26.6421, avg. loss [1901-2000]: 0.0119]</code></pre>
<pre class="python"><code>f, (a1, a2) = plt.subplots(ncols=2, sharey=True, figsize=(10, 4))
a1.set_title(&#39;MCMC&#39;)
a2.set_title(&#39;SVI&#39;)

a_line, *_ = a1.plot(mcmc_samples[&#39;a_n&#39;].T, c=&#39;b&#39;, alpha=0.1, label=&#39;[A]&#39;)
a_line.set_alpha(1)
cat_line, *_ = a1.plot(mcmc_samples[&#39;cat&#39;].T, c=&#39;r&#39;, alpha=0.1, label=&#39;[cat]&#39;)
cat_line.set_alpha(1)
a2.plot(svi_samples[&#39;a_n&#39;].T, c=&#39;b&#39;, alpha=0.1)
a2.plot(svi_samples[&#39;cat&#39;].T, c=&#39;r&#39;, alpha=0.1)
f.legend(handles=[a_line, cat_line])
a_line.set_alpha(0.1)
cat_line.set_alpha(0.1)</code></pre>
<figure>
<img src="2025-11-26-reparam_files/2025-11-26-reparam_6_0.png"
alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<p>Very nice, we get a range of <span
class="math inline">[<em>A</em>]</span> trajectories based on possible
<span class="math inline"><em>k</em></span>’s but also <span
class="math inline"><em>k</em><sub><em>d</em></sub></span>’s. But now
let’s just sample simulate adding measurement error as a
<code>LogNormal</code>. The previous variables shouldn’t be affected
because we are not making an observation. What’s notable about this
<code>a_draw</code> variable is that it has a very narrow distribution
(<span class="math inline"><em>σ</em></span> = 0.02).</p>
<pre class="python"><code>def model(a_0, cat_0, dt, n, err):
    k = sample(&#39;k&#39;, dist.LogNormal(0.0, 0.5))
    k_d = sample(&#39;k_d&#39;, dist.LogNormal(-5.0, 0.5))
    cats = deterministic(&#39;cat&#39;, solve_cat(cat_0, k_d, dt, n))
    a_n = deterministic(&#39;a_n&#39;, solve(a_0, k, cats, dt))
    a_draw = sample(&#39;a_draw&#39;, dist.LogNormal(jnp.log(a_n), err))

args = (1.0, 1.0, 0.01, 500, 0.02)
mcmc = MCMC(NUTS(model), num_warmup=1000, num_samples=100)
mcmc.run(PRNGKey(0), *args)
mcmc_samples = mcmc.get_samples()

guide = autoguide.AutoNormal(model)
svi = SVI(model, guide, Adam(0.01), Trace_ELBO())
svi_result = svi.run(PRNGKey(1), 2000, *args)
svi_samples = guide.sample_posterior(PRNGKey(0), svi_result.params, sample_shape=(100,))</code></pre>
<pre><code>sample: 100%|██████████| 1100/1100 [00:01&lt;00:00, 663.80it/s, 63 steps of size 4.19e-02. acc. prob=0.92]
100%|██████████| 2000/2000 [00:00&lt;00:00, 3496.48it/s, init loss: 6657175.0000, avg. loss [1901-2000]: 1581.4250]</code></pre>
<pre class="python"><code>f, (a1, a2) = plt.subplots(ncols=2, sharey=True, figsize=(10, 4))
a1.set_title(&#39;MCMC&#39;)
a2.set_title(&#39;SVI&#39;)

a_line, *_ = a1.plot(mcmc_samples[&#39;a_n&#39;].T, c=&#39;b&#39;, alpha=0.1, label=&#39;[A]&#39;)
a_line.set_alpha(1)
cat_line, *_ = a1.plot(mcmc_samples[&#39;cat&#39;].T, c=&#39;r&#39;, alpha=0.1, label=&#39;[cat]&#39;)
cat_line.set_alpha(1)
meas_line, *_ = a1.plot(mcmc_samples[&#39;a_draw&#39;].T, c=&#39;k&#39;, alpha=0.02, label=&#39;[A] measured&#39;)
meas_line.set_alpha(1)
a2.plot(svi_samples[&#39;a_n&#39;].T, c=&#39;b&#39;, alpha=0.1)
a2.plot(svi_samples[&#39;cat&#39;].T, c=&#39;r&#39;, alpha=0.1)
a2.plot(svi_samples[&#39;a_draw&#39;].T, c=&#39;k&#39;, alpha=0.02)
f.legend(handles=[a_line, cat_line, meas_line])
a_line.set_alpha(0.1)
cat_line.set_alpha(0.1)
meas_line.set_alpha(0.02)</code></pre>
<figure>
<img src="2025-11-26-reparam_files/2025-11-26-reparam_9_0.png"
alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<p>Yes, not looking good at all. We were expecting the exact sample
plots! Increasing the number of samples, warmup, SVI steps doesn’t help
either. The culprit is clearly the narrow distribution as increasing its
std to 1.0 everything goes back to normal.</p>
<p>After a bit of head scratching, it turns out there is a way to fix
this without rewriting the model and manually de-centering/scaling. This
happens on the level of the <code>Normal</code> distributions being
sampled with (<span class="math inline"><em>μ</em> = 0</span> and <span
class="math inline"><em>σ</em> = 1.0</span>) but the
<code>LogNormal</code> distribution is a further exponential transform
away from so that’s where <code>TransformReparam</code> comes in.</p>
<pre class="python"><code>reparam_config = {
    &#39;k&#39;: reparam.TransformReparam(),
    &#39;k_d&#39;: reparam.TransformReparam(),
    &#39;a_draw&#39;: reparam.TransformReparam(),
    &#39;k_base&#39;: reparam.LocScaleReparam(0),
    &#39;k_d_base&#39;: reparam.LocScaleReparam(0),
    &#39;a_draw_base&#39;: reparam.LocScaleReparam(0),
}

def model(a_0, cat_0, dt, n, err):
    k = sample(&#39;k&#39;, dist.LogNormal(0.0, 0.5))
    k_d = sample(&#39;k_d&#39;, dist.LogNormal(-5.0, 0.5))
    cats = deterministic(&#39;cat&#39;, solve_cat(cat_0, k_d, dt, n))
    a_n = deterministic(&#39;a_n&#39;, solve(a_0, k, cats, dt))
    a_draw = sample(&#39;a_draw&#39;, dist.LogNormal(jnp.log(a_n), err))

model = handlers.reparam(model, reparam_config)

args = (1.0, 1.0, 0.01, 500, 0.02)
mcmc = MCMC(NUTS(model), num_warmup=1000, num_samples=100)
mcmc.run(PRNGKey(0), *args)
mcmc_samples = mcmc.get_samples()

guide = autoguide.AutoNormal(model)
svi = SVI(model, guide, Adam(0.01), Trace_ELBO())
svi_result = svi.run(PRNGKey(1), 2000, *args)
svi_samples = guide.sample_posterior(PRNGKey(0), svi_result.params, sample_shape=(100,))</code></pre>
<pre><code>sample: 100%|██████████| 1100/1100 [00:00&lt;00:00, 1678.97it/s, 15 steps of size 3.03e-01. acc. prob=0.89]
100%|██████████| 2000/2000 [00:00&lt;00:00, 4569.60it/s, init loss: 1233.4703, avg. loss [1901-2000]: 2.5142]</code></pre>
<pre class="python"><code>f, (a1, a2) = plt.subplots(ncols=2, sharey=True, figsize=(10, 4))
a1.set_title(&#39;MCMC&#39;)
a2.set_title(&#39;SVI&#39;)

a_line, *_ = a1.plot(mcmc_samples[&#39;a_n&#39;].T, c=&#39;b&#39;, alpha=0.1, label=&#39;[A]&#39;)
a_line.set_alpha(1)
cat_line, *_ = a1.plot(mcmc_samples[&#39;cat&#39;].T, c=&#39;r&#39;, alpha=0.1, label=&#39;[cat]&#39;)
cat_line.set_alpha(1)
meas_line, *_ = a1.plot(mcmc_samples[&#39;a_draw&#39;].T, c=&#39;k&#39;, alpha=0.05, label=&#39;[A] measured&#39;)
meas_line.set_alpha(1)
a2.plot(svi_samples[&#39;a_n&#39;].T, c=&#39;b&#39;, alpha=0.1)
a2.plot(svi_samples[&#39;cat&#39;].T, c=&#39;r&#39;, alpha=0.1)
a2.plot(svi_samples[&#39;a_draw&#39;].T, c=&#39;k&#39;, alpha=0.05)
f.legend(handles=[a_line, cat_line, meas_line])
a_line.set_alpha(0.1)
cat_line.set_alpha(0.1)
meas_line.set_alpha(0.02)</code></pre>
<figure>
<img src="2025-11-26-reparam_files/2025-11-26-reparam_12_0.png"
alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<p>Beautiful! The latent concentrations are now exactly as before and we
also have a nice extra variable showing measurement with error.</p>
<p>You will notice a quirk: I had to know that the name of the
underlying <code>Normal</code> site Now one downside of using this
automatic reparameterisation is that <code>TransformReparam</code>
doesn’t currently support observations. That’s fine though, you can
split the sampled and observed parts (the observed part won’t need
reparameterising as it won’t need to be sampled).</p>
<p>Anyway, depending on your background this may seem very dense or
trivial and I wish there was more time to expand and properly discuss
but that will have to wait util another time.</p></div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.7.2/highlight.min.js"></script>
    <script>
      hljs.highlightAll();
    </script>
  </body>
</html>
