<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title></title>
    <link
      rel="alternate"
      type="application/rss+xml"
      title="Hessam's blog RSS Feed"
      href="/feed.xml"
    />
    <link rel="stylesheet" href="/style.css" />
    <link rel="stylesheet" href="/primer.css" />
    <link
      rel="stylesheet"
      href="/light.css"
      media="(prefers-color-scheme: light)"
    />
    <link
      rel="stylesheet"
      href="/dark.css"
      media="(prefers-color-scheme: dark)"
    />
  </head>
  <body>
    <div class="container-lg px-3 my-5 markdown-body"><h1
id="amortized-probabilistic-models-for-chemical-microscopy">Amortized
probabilistic models for chemical microscopy</h1>
<h3
id="can-we-used-an-amortized-model-to-speed-up-inference-in-our-droplet-microscopy-model">Can
we used an amortized model to speed up inference in our droplet
microscopy model?</h3>
<p>My last go using a probabilistic model to analyze a microscope image
seemed to work well enough, but I wanted to take a more flexible
approach to modelling the appearance of droplets without having to roll
out a more sophisticated physical model. Also, it seemed impractical to
require a beefy GPU and minutes of compute for a single image.</p>
<p>Iâ€™ve been meaning to experiment with <em>amortized</em> inference a
bit more recently. The idea is that instead of all latent variables
being inferred, a small model (think a miniature multi-layer perceptron)
is trained to predict a subset of these variables. We are interested in
inferring droplet locations and compositions, not so much about
rendering the droplets themselves. This approach can give the best of
both worlds, a mechanistic interpretable model for the parts that we
care about or are easy to reason about, and an data-driven, learned
representation for the complex, inherently introspectable parts.</p>
<pre class="python"><code>import jax
import jax.numpy as jnp
import jax.nn as jnn
import flax.linen as nn
import matplotlib.pyplot as plt
import numpy as np
import numpyro.distributions as dist
import seaborn as sns
from numpyro import deterministic, plate, sample
from numpyro.handlers import seed, trace, substitute
from numpyro.infer import SVI, Trace_ELBO, MCMC, NUTS
from numpyro.infer.autoguide import AutoNormal
from numpyro.optim import Adam
from PIL import Image

plt.rcParams[&#39;figure.dpi&#39;] = 200

sns.set_theme(context=&#39;paper&#39;, style=&#39;ticks&#39;, font=&#39;Arial&#39;)</code></pre>
<p>Weâ€™ll use the same delightful microscope image as last time, part of
the experiments that went into our <a
href="https://doi.org/10.1039/D5DD00100E">latest paper</a>.</p>
<pre class="python"><code>img = Image.open(&#39;data/example.jpg&#39;)
img = img.resize((img.width // 4, img.height // 4))
img</code></pre>
<figure>
<img
src="2025-07-21-droplet-generative-process-3_files/2025-07-21-droplet-generative-process-3_3_0.png"
alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<pre class="python"><code>img = np.array(img) / 255.0</code></pre>
<pre class="python"><code>class DropletOpticsModel(nn.Module):
    hidden_dims: tuple = (32, 16, 8)
    
    @nn.compact
    def __call__(self, background, dx, dy, radius, composition):
        &quot;&quot;&quot;
        Args:
            background: (batch, n_channels) - existing/background pixel values
            dx: (batch, 1) - normalized distance from droplet center in x-direction
            dy: (batch, 1) - normalized distance from droplet center in y-direction
            radius: (batch, 1) - droplet radius
            composition: (batch, n_composition_features) - droplet composition vector
        
        Returns:
            new_pixel_value: (batch, n_channels) - predicted new pixel values
        &quot;&quot;&quot;
        # Concatenate all input features
        x = jnp.concatenate([background, dx, dy, radius, composition], axis=-1)

        for i, dim in enumerate(self.hidden_dims):
            x = nn.Dense(dim, name=f&#39;hidden_{i}&#39;)(x)
            x = nn.LayerNorm(name=f&#39;ln_{i}&#39;)(x)
            x = jnn.relu(x)
        
        # Output layer - predict change in pixel values
        delta = nn.Dense(background.shape[-1], name=&#39;output&#39;)(x)
        
        # Add residual connection and apply sigmoid to keep values in [0, 1]
        new_pixel_value = jnn.sigmoid(background + delta)
        
        return new_pixel_value

# Initialize model
model = DropletOpticsModel()

# Test with dummy data
key = jax.random.PRNGKey(42)
batch_size, n_channels, n_composition = 32, 3, 10

dummy_background = jax.random.uniform(key, (batch_size, n_channels))
dummy_dx = jax.random.uniform(key, (batch_size, 1), minval=-1.0, maxval=1.0)
dummy_dy = jax.random.uniform(key, (batch_size, 1), minval=-1.0, maxval=1.0)
dummy_radius = jax.random.uniform(key, (batch_size, 1)) * 0.1  # Small radii
dummy_composition = jax.random.uniform(key, (batch_size, n_composition))

# Initialize parameters
print(f&quot;{dummy_background.shape=}, {dummy_dx.shape=}, {dummy_dy.shape=}, {dummy_radius.shape=}, {dummy_composition.shape=}&quot;)
params = model.init(key, dummy_background, dummy_dx, dummy_dy, dummy_radius, dummy_composition)

# Test forward pass
output = model.apply(params, dummy_background, dummy_dx, dummy_dy, dummy_radius, dummy_composition)
print(f&quot;Input background shape: {dummy_background.shape}&quot;)
print(f&quot;Output pixel values shape: {output.shape}&quot;)
print(f&quot;Output range: [{output.min():.3f}, {output.max():.3f}]&quot;)

# Print model summary
print(f&quot;\nModel summary:&quot;)
print(model.tabulate(key, dummy_background, dummy_dx, dummy_dy, dummy_radius, dummy_composition))</code></pre>
<pre><code>dummy_background.shape=(32, 3), dummy_dx.shape=(32, 1), dummy_dy.shape=(32, 1), dummy_radius.shape=(32, 1), dummy_composition.shape=(32, 10)
Input background shape: (32, 3)
Output pixel values shape: (32, 3)
Output range: [0.260, 0.897]

Model summary:

[3m                           DropletOpticsModel Summary                           [0m
â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“
â”ƒ[1m [0m[1mpath    [0m[1m [0mâ”ƒ[1m [0m[1mmodule        [0m[1m [0mâ”ƒ[1m [0m[1minputs        [0m[1m [0mâ”ƒ[1m [0m[1moutputs       [0m[1m [0mâ”ƒ[1m [0m[1mparams        [0m[1m [0mâ”ƒ
â”¡â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©
â”‚          â”‚ DropletOpticsâ€¦ â”‚ -              â”‚ [2mfloat32[0m[32,3]  â”‚                â”‚
â”‚          â”‚                â”‚ [2mfloat32[0m[32,3]  â”‚                â”‚                â”‚
â”‚          â”‚                â”‚ -              â”‚                â”‚                â”‚
â”‚          â”‚                â”‚ [2mfloat32[0m[32,1]  â”‚                â”‚                â”‚
â”‚          â”‚                â”‚ -              â”‚                â”‚                â”‚
â”‚          â”‚                â”‚ [2mfloat32[0m[32,1]  â”‚                â”‚                â”‚
â”‚          â”‚                â”‚ -              â”‚                â”‚                â”‚
â”‚          â”‚                â”‚ [2mfloat32[0m[32,1]  â”‚                â”‚                â”‚
â”‚          â”‚                â”‚ -              â”‚                â”‚                â”‚
â”‚          â”‚                â”‚ [2mfloat32[0m[32,10] â”‚                â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ hidden_0 â”‚ Dense          â”‚ [2mfloat32[0m[32,16] â”‚ [2mfloat32[0m[32,32] â”‚ bias:          â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [2mfloat32[0m[32]    â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ kernel:        â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [2mfloat32[0m[16,32] â”‚
â”‚          â”‚                â”‚                â”‚                â”‚                â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [1m544 [0m[1;2m(2.2 KB)[0m   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ln_0     â”‚ LayerNorm      â”‚ [2mfloat32[0m[32,32] â”‚ [2mfloat32[0m[32,32] â”‚ bias:          â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [2mfloat32[0m[32]    â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ scale:         â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [2mfloat32[0m[32]    â”‚
â”‚          â”‚                â”‚                â”‚                â”‚                â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [1m64 [0m[1;2m(256 B)[0m     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ hidden_1 â”‚ Dense          â”‚ [2mfloat32[0m[32,32] â”‚ [2mfloat32[0m[32,16] â”‚ bias:          â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [2mfloat32[0m[16]    â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ kernel:        â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [2mfloat32[0m[32,16] â”‚
â”‚          â”‚                â”‚                â”‚                â”‚                â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [1m528 [0m[1;2m(2.1 KB)[0m   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ln_1     â”‚ LayerNorm      â”‚ [2mfloat32[0m[32,16] â”‚ [2mfloat32[0m[32,16] â”‚ bias:          â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [2mfloat32[0m[16]    â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ scale:         â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [2mfloat32[0m[16]    â”‚
â”‚          â”‚                â”‚                â”‚                â”‚                â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [1m32 [0m[1;2m(128 B)[0m     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ hidden_2 â”‚ Dense          â”‚ [2mfloat32[0m[32,16] â”‚ [2mfloat32[0m[32,8]  â”‚ bias:          â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [2mfloat32[0m[8]     â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ kernel:        â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [2mfloat32[0m[16,8]  â”‚
â”‚          â”‚                â”‚                â”‚                â”‚                â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [1m136 [0m[1;2m(544 B)[0m    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ln_2     â”‚ LayerNorm      â”‚ [2mfloat32[0m[32,8]  â”‚ [2mfloat32[0m[32,8]  â”‚ bias:          â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [2mfloat32[0m[8]     â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ scale:         â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [2mfloat32[0m[8]     â”‚
â”‚          â”‚                â”‚                â”‚                â”‚                â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [1m16 [0m[1;2m(64 B)[0m      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ output   â”‚ Dense          â”‚ [2mfloat32[0m[32,8]  â”‚ [2mfloat32[0m[32,3]  â”‚ bias:          â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [2mfloat32[0m[3]     â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ kernel:        â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [2mfloat32[0m[8,3]   â”‚
â”‚          â”‚                â”‚                â”‚                â”‚                â”‚
â”‚          â”‚                â”‚                â”‚                â”‚ [1m27 [0m[1;2m(108 B)[0m     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚[1m [0m[1m        [0m[1m [0mâ”‚[1m [0m[1m              [0m[1m [0mâ”‚[1m [0m[1m              [0m[1m [0mâ”‚[1m [0m[1m         Total[0m[1m [0mâ”‚[1m [0m[1m1,347 [0m[1;2m(5.4 KB)[0m[1m [0mâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
[1m                                                                                [0m
[1m                        Total Parameters: 1,347 [0m[1;2m(5.4 KB)[0m[1m                        [0m</code></pre>
<p>What ended up working in the end is when each pixel refers to its
closest droplet for colour. Not ideal but it keeps the memory
requirement manageable.</p>
<pre class="python"><code>def tree_to_dists(tree, path=&#39;&#39;):
    if isinstance(tree, dict):
        return {k: tree_to_dists(v, path + &#39;/&#39; + k) for k, v in tree.items()}
    else:
        # print(f&quot;Sampling {path} with shape {tree.shape}&quot;)
        return sample(path, dist.Normal().expand(tree.shape))</code></pre>
<pre class="python"><code>def model_with_nn(img, n_droplets, types=10):
    h, w, n_channels = img.shape
    
    # Create coordinate grids
    y_coords, x_coords = jnp.mgrid[:h, :w]
    
    # Sample background per channel
    with plate(&quot;channels&quot;, n_channels):
        bg = sample(&quot;bg&quot;, dist.Uniform(0, 1).expand((n_channels,)))

    # Sample droplet parameters
    with plate(&quot;droplets&quot;, n_droplets):
        x = sample(&quot;x&quot;, dist.Uniform(0, w))
        y = sample(&quot;y&quot;, dist.Uniform(0, h))
        r = sample(&quot;r&quot;, dist.LogNormal(0, 0.5))
        with plate(&quot;types&quot;, types):
            composition = sample(&quot;composition&quot;, dist.Uniform(0, 1)).T

    model = DropletOpticsModel()

    # Initialize background image
    prediction = jnp.broadcast_to(bg, (h, w, n_channels))
    nn_params = model.init(key, 
                           jnp.zeros((h * w, n_channels)), 
                           jnp.zeros((h * w, 1)),
                           jnp.zeros((h * w, 1)),
                           jnp.zeros((h * w, 1)),
                           jnp.zeros((h * w, types)))    
    nn_params = tree_to_dists(nn_params, path=&#39;nn_params&#39;)

    distance = ((x_coords[..., None] - x) / r)**2 + ((y_coords[..., None] - y) / r)**2
    nearest = jnp.argmin(distance, axis=-1)

    # Calculate relative distances from droplet center
    dx = (x_coords - x[nearest]) / r[nearest]  # Normalized by radius
    dy = (y_coords - y[nearest]) / r[nearest]  # Normalized by radius


    # Flatten spatial dimensions for neural network processing
    dx_flat = dx.flatten()[:, None]
    dy_flat = dy.flatten()[:, None]
    r_flat = r[nearest].flatten()[:, None]

    
    # Repeat background and composition for all pixels
    bg_flat = prediction.reshape(-1, n_channels)
    comp_flat = composition[nearest, :].reshape(-1, types)
    
    # Apply neural network to get new pixel values
    prediction = model.apply(nn_params, bg_flat, dx_flat, dy_flat, r_flat, comp_flat)
    
    # Reshape back to image dimensions
    prediction = prediction.reshape(h, w, n_channels)
    
    prediction = jnp.clip(prediction, 0, 1)
    prediction = deterministic(&#39;prediction&#39;, prediction)
    diff = deterministic(&#39;diff&#39;, img - prediction)
    sample(&#39;obs&#39;, dist.Normal(scale=0.05), obs=diff)
    # print(f&quot;{x_coords.shape=}, {y_coords.shape=}, {x.shape=}, {y.shape=}, {r.shape=}, {composition.shape=}, {distance.shape=}, {nearest.shape=}, {dx.shape=}, {dy.shape=}, {dx_flat.shape=}, {dy_flat.shape=}, {r_flat.shape=}, {bg_flat.shape=}, {comp_flat.shape=}, {prediction.shape=}&quot;)
    return nn_params</code></pre>
<pre class="python"><code>tr = trace(seed(model_with_nn, 0)).get_trace(img, 1000, types=5)
nn_params = seed(model_with_nn, 0)(img, 1000, types=5)
{k: v[&#39;value&#39;].shape for k, v in tr.items() if &#39;value&#39; in v}</code></pre>
<pre><code>{&#39;channels&#39;: (3,),
 &#39;bg&#39;: (3,),
 &#39;droplets&#39;: (1000,),
 &#39;x&#39;: (1000,),
 &#39;y&#39;: (1000,),
 &#39;r&#39;: (1000,),
 &#39;types&#39;: (5,),
 &#39;composition&#39;: (5, 1000),
 &#39;nn_params/params/hidden_0/kernel&#39;: (11, 32),
 &#39;nn_params/params/hidden_0/bias&#39;: (32,),
 &#39;nn_params/params/ln_0/scale&#39;: (32,),
 &#39;nn_params/params/ln_0/bias&#39;: (32,),
 &#39;nn_params/params/hidden_1/kernel&#39;: (32, 16),
 &#39;nn_params/params/hidden_1/bias&#39;: (16,),
 &#39;nn_params/params/ln_1/scale&#39;: (16,),
 &#39;nn_params/params/ln_1/bias&#39;: (16,),
 &#39;nn_params/params/hidden_2/kernel&#39;: (16, 8),
 &#39;nn_params/params/hidden_2/bias&#39;: (8,),
 &#39;nn_params/params/ln_2/scale&#39;: (8,),
 &#39;nn_params/params/ln_2/bias&#39;: (8,),
 &#39;nn_params/params/output/kernel&#39;: (8, 3),
 &#39;nn_params/params/output/bias&#39;: (3,),
 &#39;prediction&#39;: (380, 507, 3),
 &#39;diff&#39;: (380, 507, 3),
 &#39;obs&#39;: (380, 507, 3)}</code></pre>
<pre class="python"><code>guide = AutoNormal(model_with_nn)
svi = SVI(model_with_nn, guide, Adam(0.01), Trace_ELBO())

svi_result = svi.run(jax.random.PRNGKey(0), 100000, img, 800, types=3)
samples_svi = guide.sample_posterior(jax.random.PRNGKey(0), svi_result.params, sample_shape=(100,))
fig, ax = plt.subplots(figsize=(5, 2))
ax.plot(svi_result.losses)</code></pre>
<pre><code>100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100000/100000 [08:35&lt;00:00, 193.87it/s, init loss: 6371727.5000, avg. loss [95001-100000]: -854501.1250]





[&lt;matplotlib.lines.Line2D at 0x721bf0302960&gt;]</code></pre>
<figure>
<img
src="2025-07-21-droplet-generative-process-3_files/2025-07-21-droplet-generative-process-3_10_2.png"
alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<pre class="python"><code>plt.imshow(samples_svi[&#39;prediction&#39;].mean(axis=0))</code></pre>
<pre><code>&lt;matplotlib.image.AxesImage at 0x721cd022ff80&gt;</code></pre>
<figure>
<img
src="2025-07-21-droplet-generative-process-3_files/2025-07-21-droplet-generative-process-3_11_1.png"
alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<p>Not a bad reconstruction, and this time we capture color as well.</p>
<pre class="python"><code>samples_svi[&#39;composition&#39;][:1].shape</code></pre>
<pre><code>(1, 3, 800)</code></pre>
<pre class="python"><code>plt.imshow(img)

plt.scatter(samples_svi[&#39;x&#39;][0], samples_svi[&#39;y&#39;][0], s=4, alpha=1.0, c=samples_svi[&#39;composition&#39;][0].T, marker=&#39;x&#39;)</code></pre>
<pre><code>&lt;matplotlib.collections.PathCollection at 0x721c10238f80&gt;</code></pre>
<figure>
<img
src="2025-07-21-droplet-generative-process-3_files/2025-07-21-droplet-generative-process-3_14_1.png"
alt="png" />
<figcaption aria-hidden="true">png</figcaption>
</figure>
<p>Droplet composition seems nicely consistent with the image.</p>
<h2 id="other-attempts">Other attempts</h2>
<p>A couple of other approaches that didnâ€™t quite work.</p>
<h3 id="fully-flattened-model-with-aggregation">Fully flattened model
with aggregation</h3>
<p>Two issues: memory use and how to aggregate the results at the
end.</p>
<pre class="python"><code>def model_with_nn(img, n_droplets, types=10):
    h, w, n_channels = img.shape
    
    # Create coordinate grids
    y_coords, x_coords = jnp.mgrid[:h, :w]
    
    # Sample background per channel
    with plate(&quot;channels&quot;, n_channels):
        bg = sample(&quot;bg&quot;, dist.Uniform(0, 1).expand((n_channels,)))

    # Sample droplet parameters
    with plate(&quot;droplets&quot;, n_droplets):
        x = sample(&quot;x&quot;, dist.Uniform(0, w))
        y = sample(&quot;y&quot;, dist.Uniform(0, h))
        r = sample(&quot;r&quot;, dist.LogNormal(0, 0.5))
        with plate(&quot;types&quot;, types):
            composition = sample(&quot;composition&quot;, dist.Uniform(0, 1)).T

    model = DropletOpticsModel()

    # Initialize background image
    bg = jnp.broadcast_to(bg, (h, w, n_channels))
    nn_params = model.init(key, 
                           jnp.zeros((h * w, n_channels)), 
                           jnp.zeros((h * w, 1)),
                           jnp.zeros((h * w, 1)),
                           jnp.zeros((h * w, 1)),
                           jnp.zeros((h * w, types)))    
    nn_params = tree_to_dists(nn_params, path=&#39;nn_params&#39;)
    
    # Calculate relative distances from droplet center
    dx = (x_coords[..., None] - x) / r  # Normalized by radius
    dy = (y_coords[..., None] - y) / r  # Normalized by radius

    # Flatten spatial dimensions for neural network processing
    dx_flat = dx.flatten()[:, None]
    dy_flat = dy.flatten()[:, None]
    r_flat = jnp.broadcast_to(r, (h, w, n_droplets)).flatten()[:, None]
    
    # Repeat background and composition for all pixels
    bg_flat = jnp.broadcast_to(bg[:, :, None, :], (h, w, n_droplets, n_channels)).reshape(-1, n_channels)
    comp_flat = jnp.broadcast_to(composition, (h * w, n_droplets, types)).reshape(-1, types)
    
    # Apply neural network to get new pixel values
    new_pixels = model.apply(nn_params, bg_flat, dx_flat, dy_flat, r_flat, comp_flat)
    
    # Reshape back to image dimensions
    new_pixels = new_pixels.reshape(h, w, n_channels)
    
    # Update prediction (could be additive or replacement - using replacement here)
    prediction = new_pixels
    
    prediction = jnp.clip(prediction, 0, 1)
    prediction = deterministic(&#39;prediction&#39;, prediction)
    diff = deterministic(&#39;diff&#39;, img - prediction)
    sample(&#39;obs&#39;, dist.Normal(scale=0.05), obs=diff)</code></pre>
<h3 id="iterative-with-lax.scan">Iterative with
<code>lax.scan</code></h3>
<pre class="python"><code>def model_with_nn(img, n_droplets, types=10):
    h, w, n_channels = img.shape
    
    # Create coordinate grids
    y_coords, x_coords = jnp.mgrid[:h, :w]
    
    # Sample background per channel
    with plate(&quot;channels&quot;, n_channels):
        bg = sample(&quot;bg&quot;, dist.Uniform(0, 1).expand((n_channels,)))

    # Sample droplet parameters
    with plate(&quot;droplets&quot;, n_droplets):
        x = sample(&quot;x&quot;, dist.Uniform(0, w))
        y = sample(&quot;y&quot;, dist.Uniform(0, h))
        r = sample(&quot;r&quot;, dist.LogNormal(0, 0.5))
        with plate(&quot;types&quot;, types):
            composition = sample(&quot;composition&quot;, dist.Uniform(0, 1))

    model = DropletOpticsModel()

    # Initialize background image
    prediction = jnp.broadcast_to(bg, (h, w, n_channels))
    nn_params = model.init(key, 
                           jnp.zeros((h * w, n_channels)), 
                           jnp.zeros((h * w, 1)),
                           jnp.zeros((h * w, 1)),
                           jnp.zeros((h * w, 1)),
                           jnp.zeros((h * w, types)))    
    nn_params = tree_to_dists(nn_params, path=&#39;nn_params&#39;)
    # For each droplet, compute its effect using the neural network
    def apply_droplet(carry, droplet_params):
        current_prediction = carry
        x_i, y_i, r_i, comp_i = droplet_params
        
        # Calculate relative distances from droplet center
        dx = (x_coords - x_i) / r_i  # Normalized by radius
        dy = (y_coords - y_i) / r_i  # Normalized by radius
        
        # Flatten spatial dimensions for neural network processing
        dx_flat = dx.flatten()[:, None]
        dy_flat = dy.flatten()[:, None]
        r_flat = jnp.full((h * w, 1), r_i)
        
        # Repeat background and composition for all pixels
        bg_flat = current_prediction.reshape(-1, n_channels)
        comp_flat = jnp.broadcast_to(comp_i, (h * w, types))
        
        # Apply neural network to get new pixel values
        new_pixels = model.apply(nn_params, bg_flat, dx_flat, dy_flat, r_flat, comp_flat)
        
        # Reshape back to image dimensions
        new_pixels = new_pixels.reshape(h, w, n_channels)
        
        return new_pixels, None
    
    prediction, _ = jax.lax.scan(apply_droplet, prediction, (x, y, r, composition.T))
    prediction = jnp.clip(prediction, 0, 1)
    prediction = deterministic(&#39;prediction&#39;, prediction)
    diff = deterministic(&#39;diff&#39;, img - prediction)
    sample(&#39;obs&#39;, dist.Normal(scale=0.05), obs=diff)
    return nn_params</code></pre></div>
    <script src="/highlight.min.js"></script>
    <script>
      hljs.highlightAll();
    </script>
  </body>
</html>
